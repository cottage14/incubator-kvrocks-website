"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[1477],{10:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"go-redis-kvrocks-opentelemetry","metadata":{"permalink":"/blog/go-redis-kvrocks-opentelemetry","editUrl":"https://github.com/apache/incubator-kvrocks-website/tree/main/blog/2022-11-20-go-redis-kvrocks-opentelemetry/index.md","source":"@site/blog/2022-11-20-go-redis-kvrocks-opentelemetry/index.md","title":"Getting started with Kvrocks and go-redis","description":"This post explains how to get started with Apache Kvrocks using go-redis client. It also demonstrates how you can use OpenTelemetry and Uptrace to monitor Kvrocks on both client and server sides.","date":"2022-11-20T00:00:00.000Z","formattedDate":"November 20, 2022","tags":[],"readingTime":3.98,"hasTruncateMarker":true,"authors":[{"name":"Vladimir Mihailenco","title":"Grumpy Gopher","url":"https://github.com/vmihailenco","imageURL":"https://github.com/vmihailenco.png","key":"vmihailenco"}],"frontMatter":{"slug":"go-redis-kvrocks-opentelemetry","title":"Getting started with Kvrocks and go-redis","authors":["vmihailenco"]},"nextItem":{"title":"How we use RocksDB in Kvrocks?","permalink":"/blog/how-we-use-rocksdb-in-kvrocks"}},"content":"This post explains how to get started with Apache Kvrocks using go-redis client. It also demonstrates how you can use OpenTelemetry and Uptrace to monitor Kvrocks on both client and server sides.\\n\\n\x3c!--truncate--\x3e\\n\\n## What is Kvrocks?\\n\\n[Apache Kvrocks](https://kvrocks.apache.org/) is a distributed key-value NoSQL database that uses RocksDB as a storage engine and is compatible with Redis protocol.\\n\\nYou can use Kvrocks as a drop-in replacement for Redis to store data on SSD decreasing the cost of memory and increasing the capacity. For example, imagine taking one of the many existing Redis-based job queues and using them with Kvrocks and SSD storage.\\n\\nKvrocks supports most [Redis commands](/docs/supported-commands) with a single notable exception being that `watch` and `unwatch` commands are not [supported yet](https://github.com/apache/incubator-kvrocks/issues/315). (Note: At the time of writing this article, `watch`/`unwatch` was not supported. However, it is [now supported](https://github.com/apache/incubator-kvrocks/pull/1279) and expected to be released in version 2.4.0 and later.)\\n\\n[Kvrocks Cluster](/docs/cluster) and [replication](/docs/replication) are available as well.\\n\\n## Getting started with Kvrocks\\n\\nYou can launch Kvrocks using Docker:\\n\\n```shell\\ndocker run -it -p 6666:6666 apache/kvrocks\\n```\\n\\nAnd start using it right away:\\n\\n```text\\nredis-cli -p 6666\\n\\n127.0.0.1:6666> get foo\\n(nil)\\n127.0.0.1:6666> set foo bar\\nOK\\n127.0.0.1:6666> get foo\\n\\"bar\\"\\n```\\n\\nYou can also [build](https://github.com/apache/incubator-kvrocks#build-and-run-kvrocks) Kvrocks with GCC yourself.\\n\\n## Connecting to Kvrocks from Go\\n\\nSince Kvrocks uses Redis-compatible protocol, you can use your favorite Redis client to work with Kvrocks, for example, [Go Redis client](https://redis.uptrace.dev/):\\n\\n```go\\npackage main\\n\\nimport (\\n\\t\\"context\\"\\n\\t\\"github.com/go-redis/redis/v8\\"\\n)\\n\\nfunc main() {\\n\\tctx := context.Background()\\n\\n\\trdb := redis.NewClient(&redis.Options{\\n\\t\\tAddr: \\"localhost:6666\\",\\n\\t})\\n\\n\\terr := rdb.Set(ctx, \\"key\\", \\"value\\", 0).Err()\\n\\tif err != nil {\\n\\t\\tpanic(err)\\n\\t}\\n\\n\\tval, err := rdb.Get(ctx, \\"key\\").Result()\\n\\tif err != nil {\\n\\t\\tpanic(err)\\n\\t}\\n\\tfmt.Println(\\"key\\", val)\\n}\\n```\\n\\n[Pipelines](https://redis.uptrace.dev/guide/go-redis-pipelines.html),[pub/sub](https://redis.uptrace.dev/guide/go-redis-pubsub.html), and even [Lua scripts](https://redis.uptrace.dev/guide/lua-scripting.html) are working as well:\\n\\n```go\\nvar incrBy = redis.NewScript(`\\nlocal key = KEYS[1]\\nlocal change = ARGV[1]\\n\\nlocal value = redis.call(\\"GET\\", key)\\nif not value then\\n  value = 0\\nend\\n\\nvalue = value + change\\nredis.call(\\"SET\\", key, value)\\n\\nreturn value\\n`)\\n```\\n\\nYou can then run the script like this:\\n\\n```go\\nkeys := []string{\\"my_counter\\"}\\nvalues := []interface{}{+1}\\nnum, err := incrBy.Run(ctx, rdb, keys, values...).Int()\\n```\\n\\n## What is OpenTelemetry?\\n\\n[OpenTelemetry](https://opentelemetry.io/) is a vendor-neutral standard that allows you to collect and export traces, logs, and metrics.\\n\\nUptrace, one of its vendor, provides a series of good documents about the terminology and usages:\\n\\n* [What is OpenTelemetry?](https://uptrace.dev/opentelemetry)\\n* [OpenTelemetry Distributed Tracing](https://uptrace.dev/opentelemetry/distributed-tracing.html)\\n* [OpenTelemetry Metrics](https://uptrace.dev/opentelemetry/metrics.html)\\n\\nOtel allows developers to collect and export telemetry data in a vendor-agnostic way. With OpenTelemetry, you can instrument your application once and then add or change vendors without changing the instrumentation, for example, here is a list [DataDog competitors](https://uptrace.dev/get/compare/datadog-competitors.html) that support OpenTelemetry.\\n\\n## What is Uptrace?\\n\\nUptrace is a [source-available APM tool](https://uptrace.dev/get/open-source-apm.html) that supports distributed tracing, metrics, and logs. You can use it to monitor applications and set up automatic alerts to receive notifications via email, Slack, Telegram, and more.\\n\\nUptrace uses OpenTelemetry to collect data and ClickHouse database to store it. ClickHouse is the only dependency.\\n\\n## Monitoring Kvrocks client\\n\\nYou can use OpenTelemetry and Uptrace together to monitor Kvrocks performance using the go-redis instrumentation:\\n\\n```go\\nimport \\"github.com/go-redis/redis/extra/redisotel/v9\\"\\n\\nrdb := redis.NewClient(&redis.Options{\\n\\tAddr: \\":6666\\",\\n})\\n\\nif err := redisotel.InstrumentTracing(rdb, redisotel.WithDBSystem(\\"kvrocks\\")); err != nil {\\n\\tpanic(err)\\n}\\nif err := redisotel.InstrumentMetrics(rdb, redisotel.WithDBSystem(\\"kvrocks\\")); err != nil {\\n\\tpanic(err)\\n}\\n```\\n\\nOnce the data reaches Uptrace, it will generate the following dashboard for you:\\n\\n![Uptrace DB dashboard](db-dashboard.png)\\n\\nSee [Getting started with Uptrace](https://uptrace.dev/get/get-started.html) for details.\\n\\n## Monitoring Kvrocks server\\n\\nYou can also configure OpenTelemetry Redis[receiver](https://uptrace.dev/opentelemetry/collector-config.html?receiver=redis) to monitor Kvrocks:\\n\\n```yaml\\nreceivers:\\n  redis/kvrocks:\\n    endpoint: \'kvrocks:6666\'\\n    collection_interval: 10s\\n```\\n\\nThe receiver works by parsing the output of `INFO` command and produces a number of useful metrics:\\n\\n![Redis Metrics](redis-metrics.png)\\n\\nSee [GitHub example](https://github.com/uptrace/uptrace/tree/master/example/kvrocks) and [OpenTelemetry Redis monitoring](https://uptrace.dev/opentelemetry/redis-monitoring.html) for details.\\n\\n## Custom metrics\\n\\nUsing OpenTelemetry Metrics API, you can even create custom Kvrocks metrics. For example, the following function parses `used_disk_percent` to create `kvrocks.used_disk_percent` metric:\\n\\n```go\\nvar re = regexp.MustCompile(`used_disk_percent:\\\\s(\\\\d+)%`)\\n\\nfunc monitorKvrocks(ctx context.Context, rdb *redis.Client) error {\\n\\tmp := global.MeterProvider()\\n\\tmeter := mp.Meter(\\"github.com/uptrace/uptrace/example/kvrocks\\")\\n\\n\\tusedDiskPct, err := meter.AsyncFloat64().Gauge(\\n\\t\\t\\"kvrocks.used_disk_percent\\",\\n\\t\\tinstrument.WithUnit(\\"%\\"),\\n\\t)\\n\\tif err != nil {\\n\\t\\treturn err\\n\\t}\\n\\n\\treturn meter.RegisterCallback(\\n\\t\\t[]instrument.Asynchronous{\\n\\t\\t\\tusedDiskPct,\\n\\t\\t},\\n\\t\\tfunc(ctx context.Context) {\\n\\t\\t\\tpct, err := getUsedDiskPercent(ctx, rdb)\\n\\t\\t\\tif err != nil {\\n\\t\\t\\t\\totel.Handle(err)\\n\\t\\t\\t}\\n\\t\\t\\tusedDiskPct.Observe(ctx, pct, semconv.DBSystemKey.String(\\"kvrocks\\"))\\n\\t\\t},\\n\\t)\\n}\\n\\nfunc getUsedDiskPercent(ctx context.Context, rdb *redis.Client) (float64, error) {\\n\\tinfo, err := rdb.Info(ctx, \\"keyspace\\").Result()\\n\\tif err != nil {\\n\\t\\treturn 0, err\\n\\t}\\n\\n\\tm := re.FindStringSubmatch(info)\\n\\tif m == nil {\\n\\t\\treturn 0, errors.New(\\"can\'t find used_disk_percent metric\\")\\n\\t}\\n\\n\\tn, err := strconv.ParseInt(m[1], 10, 64)\\n\\tif err != nil {\\n\\t\\treturn 0, err\\n\\t}\\n\\n\\treturn float64(n) / 100, nil\\n}\\n```\\n\\nThe metric looks like this in Uptrace:\\n\\n![Redis Metrics](used-disk-percent.png)\\n\\nSee [OpenTelemetry Go Metrics API](https://uptrace.dev/opentelemetry/go-metrics.html) for details.\\n\\n## Useful links\\n\\n- [Getting started with Kvrocks](/docs/getting-started)\\n- [Go Redis](https://redis.uptrace.dev/guide/go-redis.html)\\n- [Get started with Uptrace](https://uptrace.dev/get/get-started.html)"},{"id":"how-we-use-rocksdb-in-kvrocks","metadata":{"permalink":"/blog/how-we-use-rocksdb-in-kvrocks","editUrl":"https://github.com/apache/incubator-kvrocks-website/tree/main/blog/2021-12-26-how-we-use-rocksdb/index.md","source":"@site/blog/2021-12-26-how-we-use-rocksdb/index.md","title":"How we use RocksDB in Kvrocks?","description":"Kvrocks is an open-source key-value database that is based on rocksdb and compatible with Redis protocol. Intention to decrease the cost of memory and increase the capability while compared to Redis. We would focus on how we use RocksDB features to improve the performance of the Redis on disk. Hopes this helps people who want to improve performance on RocksDB.","date":"2021-12-26T00:00:00.000Z","formattedDate":"December 26, 2021","tags":[],"readingTime":7.935,"hasTruncateMarker":true,"authors":[{"name":"Hulk Lin","title":"Apache Kvrocks (Incubating) Founders","url":"https://github.com/git-hulk","imageURL":"https://github.com/git-hulk.png","key":"hulk"}],"frontMatter":{"slug":"how-we-use-rocksdb-in-kvrocks","title":"How we use RocksDB in Kvrocks?","authors":["hulk"]},"prevItem":{"title":"Getting started with Kvrocks and go-redis","permalink":"/blog/go-redis-kvrocks-opentelemetry"},"nextItem":{"title":"How to implement bitmap on RocksDB?","permalink":"/blog/how-to-implement-bitmap-on-rocksdb"}},"content":"Kvrocks is an open-source key-value database that is based on rocksdb and compatible with Redis protocol. Intention to decrease the cost of memory and increase the capability while compared to Redis. We would focus on how we use RocksDB features to improve the performance of the Redis on disk. Hopes this helps people who want to improve performance on RocksDB.\\n\\n\x3c!--truncate--\x3e\\n\\n## Background\\n\\nLet\'s have a look at how Kvrocks uses the RocksDB before introducing performance optimization. From the implementation side, Kvrocks would encode the Redis data structure into the key-values and write them into the different RocksDB\'s column families. There\'s five column family type in Kvrocks:\\n\\n* Metadata Column Family: used to store the metadata(expired, size..) for complex data structures like Hash/Set/ZSet/List, also string key-value was stored in this column family\\n* Subkey Column Family: used to store key-values for complex data structures were mentioned before\\n* ZSetScore Column Family: only store the score of the sorted set\\n* PubSub Column Family: used to store and propagate pubsub messages between the master and replicas\\n* Propagated Column Family: used to propagate commands between the master and replicas\\n\\nAlso, Kvrocks uses the RocksDB WAL to implement the replication, for more detail can see:\\n* [Kvrocks: An Open-Source Distributed Disk Key-Value Storage With Redis Protocol](https://kvrocks.medium.com/distributed-disk-key-value-storage-kvrocks-7bc5101c8585)\\n* [How to implement the Redis data structures on RocksDB](/community/data-structure-on-rocksdb)\\n\\nWe can have a glance at the Kvrocks architecture from 10,000 feet view\uff1a\\n\\n![image](architecture.jpeg)\\n\\n## How to profile RocksDB\\n\\n### Memtable Optimization\\n\\nCurrently, Kvrocks was using the SkipList Memtable. Compared with the HashSkipList Memtable, it has better performance when searching across multiple prefixes and uses less memory. Kvrocks also enabled the whole_key_filtering the option which would create a bloom filter for the key in the memtable, it can reduce the number of comparisons, thereby reducing the CPU usage during point query.\\nRelated configuration:\\n\\n```cpp\\nmetadata_opts.memtable_whole_key_filtering = true\\nmetadata_opts.memtable_prefix_bloom_size_ratio = 0.1\\n```\\n\\n### Data Block Optimization\\n\\nPreviously, Kvrocks used binary search when searching the data block, which may cause CPU cache miss and increase the CPU usage. As the point query was the most used scenario in the key-value service, so Kvrocks switched to the hash index to reduce the binary search comparisons. **Official test data shows that this feature can reduce CPU utilization by 21.8% and increase throughput by 10%, but it will increase space usage by 4.6%.** Compared with disk resources, CPU resources are more expensive. Under the trade-off, Kvrocks chose to enable the Hash index to improve the efficiency of point queries.\\n\\nRelated configuration:\\n\\n```cpp\\nBlockBasedTableOptions::data_block_index_type = DataBlockIndexType::kDataBlockBinaryAndHash\\nBlockBasedTableOptions::data_block_hash_table_util_ratio = 0.75\\n```\\n\\n### Filter/Index Block\\n\\nThe old version of RocksDB used Bloom Filter of BlockBasedFilter type by default. The basic mechanism is to generate a Filter for every 2KB of Key-Value data, and finally form a Filter array. When searching, first check the Index Block, and for the Data Block that may have the Key, then use the corresponding Filter Block to determine whether the key exists or not.\\n\\nThe new version of RocksDB optimizes the original Filter mechanism by introducing Full Filter. Each SST has a Filter, which can check whether the Key exists or not in the SST and avoid reading the Index Block. However, in the scenario with a large key number in the SST, the Filter Block and Index Block will still be larger. For 256MB SST, the size of Index and Filter Block is about 0.5MB and 5MB, which is much larger than Data Block (usually 4\u201332KB). In the most ideal case, when the Index/Filter Block is completely stored in memory, it will only be read once per SST life cycle, but when it competes with the Data Block for the Block Cache, it is likely to be re-read from the disk due to being evicted. Do it many times, resulting in very serious read amplification.\\n\\nKvrocks\' previous approach was to dynamically adjust the SST-related configuration so that the SST file will not be too large, thereby avoiding the Index/Filter Block from being too large. However, the problem with this mechanism is that when the amount of data is very large, too many SST files will take up more system resources and cause performance degradation. The new version of Kvrocks optimizes this and opens the related configuration of the Partitioned Block. The principle of the Partitioned Block is to add a secondary index to the Index/Filter Block. When reading the Index or Filter, the secondary index is first to read into the memory, and then Find the required partition Index Block according to the secondary index, and load it into the Block Cache.\\n\\nThe advantages of Partitioned Block are as follows:\\n\\n* Increase the cache hit rate: Large Index/Filter Block will pollute the cache space. The large Block will be partitioned, allowing the Index/Filter Block to be loaded at a finer granularity, thereby effectively using the cache space\\n* Improve cache efficiency: The partition Index/Filter Block will become smaller, the lock competition in the Cache will be further reduced, and the efficiency under high concurrency will be improved\\n* Reduce IO utilization: When the cache Miss of the index/filter partition, only a small partition needs to be loaded from the disk. Compared with the Index/Filter Block that reads the entire SST file, this will make the load on the disk smaller\\n\\nRelated configuration:\\n\\n```cpp\\nformat_version = 5\\nindex_type = IndexType::kTwoLevelIndexSearch\\nBlockBasedTableOptions::partition_filters = true\\ncache_index_and_filter_blocks = true\\npin_top_level_index_and_filter = true\\ncache_index_and_filter_blocks_with_high_priority = true\\npin_l0_filter_and_index_blocks_in_cache = true\\noptimize_filters_for_memory = true\\n```\\n\\n### Data compression optimization\\n\\nRocksDB compresses the data when it\'s placed on the disk. We compared and tested different compression algorithms on Kvrocks and found that different compression algorithms have a great impact on performance, especially when CPU resources are tight, which will significantly increase latency.\\n\\nThe following figure shows the test data of compression speed and compression ratio of different compression algorithms:\\n\\n![image](compression.jpeg)\\n\\nIn Kvrocks, compression is not set for the SST of the L0 and L1 layers, because these two layers have a small amount of data. Compressing the data at these levels cannot reduce a lot of disk space, but not compressing the data at these levels can save CPU. Each Compaction from L0 to L1 needs to access all files in L1. In addition, the range scan cannot use Bloom Filter, and it needs to find all files in L0. If you do not need to decompress when reading data in L0 and L1, and writing data in L0 and L1 do not need to be compressed, then these two frequent CPU-intensive operations will take up less CPU, compared to the disk space saved by compression, it is more profitable.\\n\\n**Considering the trade-off between compression speed and compression ratio, Kvrocks mainly chooses two algorithms, LZ4 and ZSTD.** For other levels, LZ4 is used because the compression algorithm is faster and the compression ratio is higher. RocksDB officially recommends using LZ4. For scenes with large data volume and low QPS, the last layer will be set to ZSTD to further reduce storage space and reduce costs. The advantage of ZSTD is that the compression ratio is higher and the compression speed is faster.\\n\\n### Cache optimization\\n\\nFor the simple data type (String type), the data is directly stored in Metadata CF, while for complex data types, only the metadata is stored in Metadata CF, and the actual data is stored in Subkey CF. Kvrocks previously allocated the same size of Block Cache to these two CFs by default. However, the online scene is complicated and the user\'s data type cannot be predicted, so it is not possible to allocate an appropriate Block Cache size to each CF in advance. If users use simple types and use complex types in different proportions, the Block Cache hit rate will decrease. Kvrocks shared the same large Block Cache to achieve a 30% improvement in the command rate of the Cache.\\nIn addition, Row Cache is also introduced to deal with the problem of hotkeys. RocksDB checks Row Cache first, then Block Cache. For scenes with hot spots, data will be stored in Row Cache first to further improve Cache utilization.\\n\\n### Key-Value Separation\\n\\nThe LSM storage engine will store the Key and Value together. During the compaction process, both Key and Value will be rewritten. When the Value is large, it will cause serious write amplification problems. In response to this problem, the  [WiscKey](https://www.usenix.org/system/files/conference/fast16/fast16-papers-lu.pdf) paper proposed a Key-Value separation scheme. Based on this paper, the industry also realized the KV separation of LSM-type storage engines, such as RocksDB\'s BlobDB, PingCAP\'s Titan engine, Quantum engine used by Baidu\'s UNDB.\\n\\nRocksDB 6.18 version re-implemented BlobDB (RocksDB\'s Key-Value separation scheme), integrated it into the main logic of RocksDB, and has been improving and optimizing BlobDB related features. **Kvrocks introduced this feature in 2.0.5 to deal with scenarios with large values. Tests show that when Kvrocks turns on the KV separation switch, for the scenario where Value is 10KB, the write performance is increased by 2.5 times, and the read performance is not attenuated; the larger the value, the greater the write performance improvement, and the write performance is improved when the Value is 50KB. 5 times.**\\n\\nRelated configuration:\\n\\n```cpp\\nColumnFamilyOptions.enable_blob_files = config_->RocksDB.enable_blob_files;\\nmin_blob_size = 4096\\nblob_file_size = 128M\\nblob_compression_type = lz4\\nenable_blob_garbage_collection = true\\nblob_garbage_collection_age_cutoff = 0.25\\nblob_garbage_collection_force_threshold = 0.8\\n```\\n\\n## Kvrocks Roadmap\\n\\n2021 is coming to an end, the related work of [Kvrocks 2.0](https://github.com/apache/kvrocks/projects/1) has been basically completed, and the plan of [Kvrocks 3.0](https://github.com/apache/kvrocks/projects/2) has also been listed on GitHub. This article lists the following two important features."},{"id":"how-to-implement-bitmap-on-rocksdb","metadata":{"permalink":"/blog/how-to-implement-bitmap-on-rocksdb","editUrl":"https://github.com/apache/incubator-kvrocks-website/tree/main/blog/2021-11-07-how-to-implement-bitmap-on-rocksdb/index.md","source":"@site/blog/2021-11-07-how-to-implement-bitmap-on-rocksdb/index.md","title":"How to implement bitmap on RocksDB?","description":"Most developers should be familiar with bitmap, in addition to the storage implementation for the bloom filter, and many databases also provide bitmap type indexes. For memory storage, the bitmap can be regarded as the special type of sparse bit array, which would not cause the read-write amplification issue (means read/write bytes far more than the request). While Redis supports bit-related operations on string types, it is a big challenge for disk KV-based storage like Kvrocks. So this article mainly discusses \\"How to reduce disk read/write amplification on RocksDB\\".","date":"2021-11-07T00:00:00.000Z","formattedDate":"November 7, 2021","tags":[],"readingTime":6.59,"hasTruncateMarker":true,"authors":[{"name":"Hulk Lin","title":"Apache Kvrocks (Incubating) Founders","url":"https://github.com/git-hulk","imageURL":"https://github.com/git-hulk.png","key":"hulk"}],"frontMatter":{"slug":"how-to-implement-bitmap-on-rocksdb","title":"How to implement bitmap on RocksDB?","authors":["hulk"]},"prevItem":{"title":"How we use RocksDB in Kvrocks?","permalink":"/blog/how-we-use-rocksdb-in-kvrocks"}},"content":"Most developers should be familiar with bitmap, in addition to the storage implementation for the bloom filter, and many databases also provide bitmap type indexes. For memory storage, the bitmap can be regarded as the special type of sparse bit array, which would not cause the read-write amplification issue (means read/write bytes far more than the request). While Redis supports bit-related operations on string types, it is a big challenge for disk KV-based storage like [Kvrocks](https://github.com/apache/incubator-kvrocks). So this article mainly discusses \\"**How to reduce disk read/write amplification on RocksDB**\\".\\n\\n\x3c!--truncate--\x3e\\n\\n## Why amplification occurs\\n\\nAmplification mainly comes from two aspects:\\n\\n* The hardware-level requires the smallest reading and writing unit\\n* How we organize the data distribution on the software-level\\n\\nTake SSD as an example, the smallest unit of reading/writing operation was the page (commonly 4KiB/8KiB/16KiB), and it would read or write one page even the request size was 1byte. Moreover, the way of SSD modification was Read-Modify-Write instead of in-place, which means SSD would read the page content and modify then write it out to another page, the old page would be reclaimed by GC. Similar to the following:\\n\\n![value-update-on-page](value-update-on-page.jpeg)\\n\\nAs we can see, a large random io was very unfriendly to the SSD disk, except for the performance issue, frequent erasing and writing will also seriously lead to the life of the SSD (random reads and writes are also unfriendly to HDDs, requiring constant seek and addressing). LSM-Tree alleviates such problems by changing random writes into sequential batch writes.\\n\\nThe read-write amplification at the software level mainly comes from the data organization method, and the degree of read-write amplification brought about by different organization methods will also vary greatly. Take RocksDB as an example here, RocksDB is Facebook based on Google LevelDB which enriches the multi-threading, Backup and Compaction, and many other very useful functions. To solve the problem of the disk write amplification, it also brings some space enlargement problems. Let\'s take a brief look at how LSM-Tree organizes data:\\n\\n![major-compaction](major-compaction.jpeg)\\n\\nLSM-Tree would create a new entry per write. For example in the above picture, the variable X is written 4 times successively, which are 0, 1, 2, 3. From the single variable X side, it was caused 4 times space amplification, those old spaces would be reclaimed on the background compaction. Similarly, deletion is achieved by inserting a record whose value is empty. The space size of each layer of LSM-Tree increases layer by layer. When the capacity reaches the limit, it will trigger compaction to merge to the next layer, and so on. Assuming that the maximum storage size of Level 0 is M Bytes, it increases layer by layer in 10 times and the maximum is 7 layers. Theoretically, the space enlargement is about 1.111111 times. Calculated as follows:\\n\\n```text\\namplification ratio = (1 + 10 + 100 +1000 + 10000 + 100000 + 1000000) * M / (1000000 * M)\\n```\\n\\nHowever, the magnification space rate is much larger than this theoretical value since the last layer generally cannot reach the maximum value. It is also mentioned in the RocksDB documentation. For details see also RockDB\'s blog [\\"Dynamic Level Size for Level-Based Compaction\\"](https://rocksdb.org/blog/2015/07/23/dynamic-level.html).\\n\\nIn addition, since RocksDB reads and writes are all based on key-value, the larger the value, the greater the read-write amplification may be. For example, suppose there is a JSON with a Value of 10 MiB. If you want to modify a field in this key, you need to read the entire JSON, modify and write it back again, which will cause huge read-write amplification. The paper [\\"WiscKey: Separating Keys from Values in SSD-conscious Storage\\"](https://www.usenix.org/system/files/conference/fast16/fast16-papers-lu.pdf) that optimizes the large Key-Value of LSM-Tree by separating key-value to reduce the write amplification problem caused by Compaction. The [titan](https://github.com/tikv/titan) project of TiKV is based on WiscKey paper to optimize RocksDB\'s write amplification in large key-value scenarios. RocksDB also implements this function in the community version, but it is still in an experimental stage.\\n\\n## Implement bitmap on RocksDB\\n\\nKvrocks is disk storage compatible with the Redis protocol implemented on RocksDB. It needs to support the bitmap data structure, so needs to implement the bitmap on RocksDB. In most scenes, the bitmap is used as sparse arrays, which means the offset written should be random, for the first time maybe 1, and the next offset maybe 1000000000 or more. Therefore, the implementation will face the above-mentioned amplification issue.\\n\\nA simple way is to regard the entire bitmap as a value, and read the value into the memory and then write it back when writing. Although this implementation is very simple, it would cause seriously amplification when the value was huge. In addition to the problem of effective space utilization, it may directly cause the entire service to be unavailable since we need to read and write back the entire value. Bitmap in Pika is such an implementation, but the maximum value is limit to 128 KiB. Limit the value size can avoid the above-mentioned extreme cases, but it will greatly affect the user scenes of bitmap.\\n\\nSince we know that the core problem is caused by a single key-value that is too large, the most direct way is to split the bitmap into multiple key-values, and control the single key-value size within a reasonable range, so the amplification is relatively under control. In the current implementation of Kvrocks, each key-value is divided into 1 KiB(8192 bits). The algorithm diagram is as follows:\\n\\n![bitmap-of-kvrocks](bitmap-of-kvrocks.jpeg)\\n\\nTake `setbit foo 8192002 1` as an example, the implementation steps are:\\n\\n* Calculate the key corresponding to the offset of `8192002`, because Kvrocks uses a value of 1 KiB, so the number of the key is `8192002/(1024*8)=1000`, so you can know the bit should be stored in the sub key `foo1000`.\\n* Then get the value corresponding to this key from RocksDB and calculate the offset in the segment, `8192002%8192` is equal to `2`, and then set the bit with the offset of 2 to 1.\\n* Finally, write the entire value back to RocksDB.\\n\\nA key point of this implementation is only read-write the limit part of the bitmap we need. Assuming that we have only executed `setbit` twice, `setbit foo 1 1` and `setbit foo 8192002 1`, then there will only read and write two keys `foo:0` and `foo:1000` in RocksDB, and the actual read-value size is only 2 KiB in total. It can be perfectly adapted to the sparse array scene like the bitmap, and will also not cause the problem of space enlargement due to sparse writing.\\n\\n> This idea is also similar to Linux\'s virtual memory/physical memory mapping strategy. For example, we request to malloc for 1GiB, and the operating system only allocates a piece of virtual memory address space. The physical memory was allocated when it is actually written will it trigger a page fault interrupt. That is, if the memory page has not been written, read-only will not cause physical memory allocation.\\n\\nGetBit is similar. It first calculates the key where the offset is located, and then reads the key from RocksDB.\\n\\n* If not exist, means that segment has not been written, returns 0 directly.\\n* If exists, read the Value and return the value of the corresponding bit.\\n\\nIn addition, the actual key-value size is also determined by the largest offset currently written. It would NOT always create a 1024 KiB key-value when there is a write. This can also help to optimize the read-write amplification problem within a single key-value in some degree. You can read [the source code of bitmap](https://github.com/apache/incubator-kvrocks/blob/unstable/src/types/redis_bitmap.cc) for more details.\\n\\n## Summary\\n\\nIt can be seen that to achieve the same thing in memory and disk was entirely different, the challenges are completely different. For disk-type services, it MUST continuously optimize the random read and write and space amplification issues. Familiar with the software was not enough, also requires to understand the hardware internal."}]}')}}]);